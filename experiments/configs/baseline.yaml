# MCRL Baseline Configuration
# Standard PPO training for diamond task with fast network
# Works on GPUs with 24GB+ VRAM (RTX 3090/4090/5090, A40, etc.)

# Environment
world_size: [64, 64, 64]  # 64³ for speed, 128³ for realism
max_episode_ticks: 18000  # 15 min @ 20Hz

# Training scale - Tuned for RTX 5090 (32GB)
# Memory bottleneck is PPO update phase, not rollout collection
num_envs: 2048            # More envs = faster rollout collection
num_steps: 64             # Shorter rollouts = smaller PPO batch
num_minibatches: 32       # More minibatches = smaller memory per gradient update
update_epochs: 4
total_timesteps: 100000000  # 100M steps
# Batch = 2048*64 = 131k, Minibatch = 131k/32 = 4k (fits in memory)

# Use optimized network (4x fewer params, 3x faster)
use_fast_network: true

# Random seed
seed: 42
run_name: ppo_5090_optimized
output_dir: experiments/runs

# Network architecture (only used if use_fast_network=false)
network:
  num_block_types: 256
  embed_dim: 64
  cnn_channels: [64, 128, 256]
  inventory_hidden: 128
  state_hidden: 64
  facing_hidden: 64
  trunk_hidden: 512
  activation: relu

# PPO hyperparameters
ppo:
  lr: 0.0003
  lr_schedule: linear
  gamma: 0.999             # High for long-horizon diamond task
  gae_lambda: 0.95
  clip_eps: 0.2
  clip_value: true
  vf_coef: 0.5
  ent_coef: 0.02           # Start higher for exploration
  ent_coef_final: 0.005    # Anneal to lower for exploitation
  ent_decay_steps: 50000000
  max_grad_norm: 0.5
  normalize_advantages: true
  clip_advantages: 10.0

# Exploration
exploration:
  use_intrinsic: true
  intrinsic_coef: 0.5
  intrinsic_decay: 0.9999
  random_action_prob: 0.05  # 5% random actions initially
  random_action_decay_steps: 10000000
  max_visit_count: 100000

# Logging
logging:
  log_interval: 10
  save_interval: 100
  eval_interval: 50
  profile_interval: 100
  max_checkpoints: 5
  save_best: true
  use_wandb: false
  wandb_project: mcrl
  verbose: true
